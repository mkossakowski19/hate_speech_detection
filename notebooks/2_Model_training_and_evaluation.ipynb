{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the process of training a classification model for a hate-speech dataset. The possible targets for each entity are:\n",
    "- 0: non-harmful\n",
    "- 1: cyberbullying (addressed towards a single person)\n",
    "- 2: hate-speech (addressed towards a public person/entity/large group)\n",
    "\n",
    "The dataset comes from the 2019 PolEval competition: http://2019.poleval.pl/index.php/tasks/task6\n",
    "\n",
    "The original training set will be split into two subsets: 75% for training the models, 25% for validation. The class responsible for that is defined in the codebase.\n",
    "\n",
    "After the splitting operation, I will perform data augmentation on the training subset for the undersampled classes 1 and 2.\n",
    "The augmentation logic will be using the Google Translate API to translate some sentences from Polish to English, and then back to Polish.\n",
    "This way, we should get the sentences having the same context, but put in a different words.\n",
    "However if this causes data duplication, I will not proceed with this technique.\n",
    "\n",
    "Every observation will be cleaned using the TextPreprocessor class defined in the codebase. The steps are:\n",
    "- putting all text to lowercase,\n",
    "- removing emojis,\n",
    "- removing user mentions (ex. @mariusz)\n",
    "- removing stop words,\n",
    "- removing URL addresses,\n",
    "- removing special characters like new line, carriage return, tab,\n",
    "- removing hashtags,\n",
    "- removing punctuation marks,\n",
    "- removing redundant spaces.\n",
    "\n",
    "I decided to use the SVM algorithm, which should be a good balance between simplicity and performance. Also, it was SVM which took the first place in the PolEval compoetition :)\n",
    "\n",
    "The text data will have to be transformed into numbers. I will use the TF-IDF vectorizer for that purpose.\n",
    "\n",
    "In the first approach, both SVM and TFIDF will be trained on default hyperparameters (I will just pass the max_features parameter to TFIDF so that we don't get more features than observations in the training dataset).\n",
    "\n",
    "In the second approach, I will perform hyperparameter tuning with Optuna, using the hyperparameters ranges specified in the \"hyperparameters.yaml\" file. I will choose the best configuration at the end.\n",
    "\n",
    "The results of both approaches will be evaluated by the TextClassificationEvaluator class defined in the codebase.\n",
    "\n",
    "Default approach, hyperparameters tuning and best hyperparams will be logged in MLflow.\n",
    "\n",
    "The approaches will be tested on the original \"test\" dataset.\n",
    "\n",
    "Considering the facts that the model is expected to be a prototype and we're dealing with the multiclass classification problem, I do not have any strict expectations regarding to the metrics. The main goal is to show a process of data preparation and modelling, and later propose a deployment strategy.\n",
    "\n",
    "The approach with best results will be chosen to be deployed as a working service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I install the googletrans package here to avoid coflicts with pytest http\n",
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from googletrans import Translator\n",
    "\n",
    "from src.text_preprocessor import TextPreprocessor\n",
    "from model_training.text_classification_evaluator import TextClassificationEvaluator\n",
    "from model_training.config_parser import ConfigParser\n",
    "from model_training.train_test_split_transformation import TrainValidationSplitTransformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(text_path: str, labels_path: str):\n",
    "    with open(text_path, \"r\") as f:\n",
    "        examples = f.readlines()\n",
    "\n",
    "    with open(labels_path, \"r\") as f:\n",
    "        labels = f.readlines()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": train_examples,\n",
    "        \"label\": train_labels,\n",
    "    })\n",
    "\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_data(\"data/train/training_set_clean_only_text.txt\", \"data/train/training_set_clean_only_tags.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dla mnie faworytem do tytułu będzie Cracovia. Zobaczymy, czy typ się sprawdzi.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@anonymized_account @anonymized_account Brawo ty Daria kibic ma być na dobre i złe\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anonymized_account @anonymized_account Super, polski premier składa kwiaty na grobach kolaborantów. Ale doczekaliśmy czasów.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anonymized_account @anonymized_account Musi. Innej drogi nie mamy.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Odrzut natychmiastowy, kwaśna mina, mam problem\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jaki on był fajny xdd pamiętam, że spóźniłam się na jego pierwsze zajęcia i to sporo i za karę kazał mi usiąść w pierwszej ławce XD\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@anonymized_account No nie ma u nas szczęścia 😉\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@anonymized_account Dawno kogoś tak wrednego nie widziałam xd\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@anonymized_account @anonymized_account Zaległości były, ale ważne czy były wezwania do zapłaty z których się klub nie wywiązał.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziński jesteś kłamcą i marnym kutasem @anonymized_account\\n</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                           text  \\\n",
       "0                                                                              Dla mnie faworytem do tytułu będzie Cracovia. Zobaczymy, czy typ się sprawdzi.\\n   \n",
       "1                                                                          @anonymized_account @anonymized_account Brawo ty Daria kibic ma być na dobre i złe\\n   \n",
       "2                               @anonymized_account @anonymized_account Super, polski premier składa kwiaty na grobach kolaborantów. Ale doczekaliśmy czasów.\\n   \n",
       "3                                                                                         @anonymized_account @anonymized_account Musi. Innej drogi nie mamy.\\n   \n",
       "4                                                                                                             Odrzut natychmiastowy, kwaśna mina, mam problem\\n   \n",
       "5                         Jaki on był fajny xdd pamiętam, że spóźniłam się na jego pierwsze zajęcia i to sporo i za karę kazał mi usiąść w pierwszej ławce XD\\n   \n",
       "6                                                                                                             @anonymized_account No nie ma u nas szczęścia 😉\\n   \n",
       "7                                                                                               @anonymized_account Dawno kogoś tak wrednego nie widziałam xd\\n   \n",
       "8                            @anonymized_account @anonymized_account Zaległości były, ale ważne czy były wezwania do zapłaty z których się klub nie wywiązał.\\n   \n",
       "9  @anonymized_account @anonymized_account @anonymized_account Gdzie jest @anonymized_account . Brudziński jesteś kłamcą i marnym kutasem @anonymized_account\\n   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  \n",
       "9      2  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10041 entries, 0 to 10040\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    10041 non-null  object\n",
      " 1   label   10041 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 157.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10041, 2)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into training and validation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_valid_transformation = TrainValidationSplitTransformation(\n",
    "    input_column_name=\"text\",\n",
    "    target_column_name=\"label\",\n",
    "    train_size=0.75,\n",
    "    do_stratify=True,\n",
    ")\n",
    "\n",
    "df_train, df_val = train_valid_transformation.split_to_training_and_val_subsets(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7530, 2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2511, 2)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.915272\n",
       "2    0.059495\n",
       "1    0.025232\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.915173\n",
       "2    0.059737\n",
       "1    0.025090\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting classes 1 and 2 - experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rephrase_text(\n",
    "    text: str,\n",
    "    translator_object: Translator = translator\n",
    ") -> str:\n",
    "    english_text = translator_object.translate(text, src=\"pl\", dest=\"en\")\n",
    "    rephrased_text = translator_object.translate(english_text, src=\"en\", dest=\"pl\").text\n",
    "    start_index = rephrased_text.index(\"text\") + 5\n",
    "    end_index = rephrased_text.index(\"wymowa\") - 2\n",
    "    return rephrased_text[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_indices = range(12)\n",
    "sample_sentences = df_train[\"text\"].values[sample_indices]\n",
    "rephrased_sentences = [rephrase_text(text) for text in sample_sentences]\n",
    "rephrasing_sample = pd.DataFrame({\n",
    "    \"original\": sample_sentences,\n",
    "    \"rephrased\": rephrased_sentences,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>rephrased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justas Lasickas z golem dla Litwy U-21, która remisuje z Wyspami Owczymi 2:2.\\n</td>\n",
       "      <td>Justas Lasickas po golu dla Litwy U-21, który zremisował z Wyspami Owczymi 2-2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@anonymized_account @anonymized_account Kostevycha ręka była rozmyślna ?\\n</td>\n",
       "      <td>@anonymized_account @anonymized_account Ręka Kostevycha była celowa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anonymized_account Ja już nie odczuwam, regularność codziennie do pracy, śmieję się wiatru w twarz, czym mocniej wieje tym głośniej sié śmieję\\n</td>\n",
       "      <td>@anonymized_account Już tego nie czuję, regularność codziennie do pracy, śmieję się w twarz wiatrowi, im mocniej wieje, tym głośniej się śmieję</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anonymized_account @anonymized_account Nikt ciebie tu nie chce rasistowski pętaku\\n</td>\n",
       "      <td>@anonymized_account @anonymized_account Nikt cię tu nie chce rasistowski draniu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#dividetourwarsaw zrób ktos live spod stadionu prosze potrzebuje tego\\n</td>\n",
       "      <td>#dividetourwarsaw czy ktoś na żywo spod stadionu proszę potrzebuję tego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@anonymized_account @anonymized_account @anonymized_account @anonymized_account @anonymized_account Kaliciaka mogli też zaprosić :)\\n</td>\n",
       "      <td>@anonymized_account @anonymized_account @anonymized_account @anonymized_account @anonymized_account Kaliciak też mógł zaprosić :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@anonymized_account Może nie było ciekawszych? :) a może ta agencja ma też znajomości w gazecie i poprosili żeby o nim wspomnieć ? :)\\n</td>\n",
       "      <td>@anonymized_account Może nie było ciekawiej? :) a może ta agencja też ma znajomości w gazecie i poprosili mnie o wzmiankę o nim? :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@anonymized_account @anonymized_account @anonymized_account Masz rację w każdym punkcie. I PiS też ma i dokładnie wie co robi i z kim ma do czynienia.\\n</td>\n",
       "      <td>@anonymized_account @anonymized_account @anonymized_account Masz rację w każdym punkcie. A PiS też ma i dokładnie wie, co robi iz kim ma do czynienia.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>W sumie to polubiłam już zajęcia z konstrukcji przekazu reklamowego i prowadzący jest świetny\\n</td>\n",
       "      <td>W sumie już mi się podobały zajęcia z budowy przekazu reklamowego i prowadzący jest super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tłit aktualny szczególnie dzisiaj‼️\\n\\nDzięki @anonymized_account \\n\\nhttps://t.co/w4nJSmo1J4\\n</td>\n",
       "      <td>Ttit aktualne szczególnie dzisiaj‼️\\n\\nDzięki @anonymized_account \\n\\nhttps://t.co/w4nJSmo1J4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@anonymized_account Tak z ciekawości chciałam zpytać czy udało mu sie odkręcić to pomyłkowe głosowanie?  Czy ktos, coś słyszal?\\n</td>\n",
       "      <td>@anonymized_account Z ciekawości chciałem zapytać, czy udało mu się odkręcić to błędne głosowanie? Ktoś coś słyszał?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@anonymized_account Wynik wynikiem, niech tu się bez kontuzji skończy.\\n</td>\n",
       "      <td>@anonymized_account Wynik jest wynikiem, niech zakończy się tutaj bez obrażeń.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    original  \\\n",
       "0                                                                            Justas Lasickas z golem dla Litwy U-21, która remisuje z Wyspami Owczymi 2:2.\\n   \n",
       "1                                                                                 @anonymized_account @anonymized_account Kostevycha ręka była rozmyślna ?\\n   \n",
       "2          @anonymized_account Ja już nie odczuwam, regularność codziennie do pracy, śmieję się wiatru w twarz, czym mocniej wieje tym głośniej sié śmieję\\n   \n",
       "3                                                                       @anonymized_account @anonymized_account Nikt ciebie tu nie chce rasistowski pętaku\\n   \n",
       "4                                                                                    #dividetourwarsaw zrób ktos live spod stadionu prosze potrzebuje tego\\n   \n",
       "5                      @anonymized_account @anonymized_account @anonymized_account @anonymized_account @anonymized_account Kaliciaka mogli też zaprosić :)\\n   \n",
       "6                    @anonymized_account Może nie było ciekawszych? :) a może ta agencja ma też znajomości w gazecie i poprosili żeby o nim wspomnieć ? :)\\n   \n",
       "7   @anonymized_account @anonymized_account @anonymized_account Masz rację w każdym punkcie. I PiS też ma i dokładnie wie co robi i z kim ma do czynienia.\\n   \n",
       "8                                                            W sumie to polubiłam już zajęcia z konstrukcji przekazu reklamowego i prowadzący jest świetny\\n   \n",
       "9                                                            Tłit aktualny szczególnie dzisiaj‼️\\n\\nDzięki @anonymized_account \\n\\nhttps://t.co/w4nJSmo1J4\\n   \n",
       "10                         @anonymized_account Tak z ciekawości chciałam zpytać czy udało mu sie odkręcić to pomyłkowe głosowanie?  Czy ktos, coś słyszal?\\n   \n",
       "11                                                                                  @anonymized_account Wynik wynikiem, niech tu się bez kontuzji skończy.\\n   \n",
       "\n",
       "                                                                                                                                                 rephrased  \n",
       "0                                                                          Justas Lasickas po golu dla Litwy U-21, który zremisował z Wyspami Owczymi 2-2.  \n",
       "1                                                                                     @anonymized_account @anonymized_account Ręka Kostevycha była celowa?  \n",
       "2          @anonymized_account Już tego nie czuję, regularność codziennie do pracy, śmieję się w twarz wiatrowi, im mocniej wieje, tym głośniej się śmieję  \n",
       "3                                                                          @anonymized_account @anonymized_account Nikt cię tu nie chce rasistowski draniu  \n",
       "4                                                                                  #dividetourwarsaw czy ktoś na żywo spod stadionu proszę potrzebuję tego  \n",
       "5                        @anonymized_account @anonymized_account @anonymized_account @anonymized_account @anonymized_account Kaliciak też mógł zaprosić :)  \n",
       "6                      @anonymized_account Może nie było ciekawiej? :) a może ta agencja też ma znajomości w gazecie i poprosili mnie o wzmiankę o nim? :)  \n",
       "7   @anonymized_account @anonymized_account @anonymized_account Masz rację w każdym punkcie. A PiS też ma i dokładnie wie, co robi iz kim ma do czynienia.  \n",
       "8                                                                W sumie już mi się podobały zajęcia z budowy przekazu reklamowego i prowadzący jest super  \n",
       "9                                                            Ttit aktualne szczególnie dzisiaj‼️\\n\\nDzięki @anonymized_account \\n\\nhttps://t.co/w4nJSmo1J4  \n",
       "10                                    @anonymized_account Z ciekawości chciałem zapytać, czy udało mu się odkręcić to błędne głosowanie? Ktoś coś słyszał?  \n",
       "11                                                                          @anonymized_account Wynik jest wynikiem, niech zakończy się tutaj bez obrażeń.  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "rephrasing_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this approach is at least worth trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_data(\n",
    "    dataframe: pd.DataFrame,\n",
    "    new_ones: int,  # how many new observations from class \"1\" we want to get\n",
    "    new_twos: int,  # how many new observations from class \"2\" we want to get\n",
    ") -> pd.DataFrame:\n",
    "    ones_number = dataframe[dataframe[\"label\"] == 1].shape[0]\n",
    "    twos_number = dataframe[dataframe[\"label\"] == 2].shape[0]\n",
    "    assert new_ones <= ones_number\n",
    "    assert new_twos <= twos_number\n",
    "    \n",
    "    ones_indices = random.sample(range(ones_number), new_ones)\n",
    "    twos_indices = random.sample(range(twos_number), new_twos)\n",
    "    \n",
    "    original_ones = dataframe[dataframe[\"label\"] == 1][\"text\"].values[ones_indices]\n",
    "    original_twos = dataframe[dataframe[\"label\"] == 2][\"text\"].values[twos_indices]\n",
    "    \n",
    "    rephrased_ones = [rephrase_text(text) for text in tqdm(original_ones)]\n",
    "    rephrased_twos = [rephrase_text(text) for text in tqdm(original_twos)]\n",
    "    \n",
    "    rephrased_ones_df = pd.DataFrame({\n",
    "        \"text\": rephrased_ones,\n",
    "        \"label\": [1 for i in range(len(rephrased_ones))]\n",
    "    })\n",
    "    rephrased_twos_df = pd.DataFrame({\n",
    "        \"text\": rephrased_twos,\n",
    "        \"label\": [2 for i in range(len(rephrased_twos))]\n",
    "    })\n",
    "    \n",
    "    return pd.concat([dataframe, rephrased_ones_df, rephrased_twos_df], axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6892\n",
       "2     448\n",
       "1     190\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create 90 new observations from class \"1\" and 215 new observations from \"2\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 90/90 [00:43<00:00,  2.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 215/215 [01:47<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train_augmented = augment_data(dataframe=df_train, new_ones=90, new_twos=215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6892\n",
       "2     663\n",
       "1     280\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_augmented[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.879643\n",
       "2    0.084620\n",
       "1    0.035737\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_augmented[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.915272\n",
       "2    0.059495\n",
       "1    0.025232\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that percentage of both undersampled classes got bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function for logging experiments into MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "\n",
    "def log_experiment(\n",
    "    experiment_name: str,\n",
    "    metrics: Dict[str, Any],\n",
    "    model: Optional[SVC] = None,\n",
    "    vectorizer: Optional[TfidfVectorizer] = None,\n",
    "    hyperparams: Optional[Dict[str, Any]] = None,\n",
    "    run_name: Optional[str] = None,\n",
    "    tag: str = Optional[None],\n",
    "):\n",
    "    \n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        if hyperparams != None:\n",
    "            for param in hyperparams:\n",
    "                mlflow.log_param(param, hyperparams[param])\n",
    "            \n",
    "        for metric in metrics:\n",
    "            mlflow.log_metric(metric, metrics[metric])\n",
    "        \n",
    "        if model != None:\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "        if vectorizer != None:\n",
    "            mlflow.sklearn.log_model(vectorizer, \"vectorizer\")\n",
    "        if tag != None:\n",
    "            mlflow.set_tag(\"tag1\", tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVM and TFIDF with default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_default = TfidfVectorizer(max_features=df_train_augmented.shape[0])\n",
    "#just to make sure we don't have more features than observations\n",
    "svm_default = SVC(verbose=1)\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "evaluator = TextClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "df_train_augmented[\"text\"] = df_train_augmented[\"text\"].apply(lambda x: preprocessor.preprocess(x))\n",
    "df_val[\"text\"] = df_val[\"text\"].apply(lambda x: preprocessor.preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_default.fit_transform(df_train_augmented[\"text\"].values)\n",
    "X_val_tfidf = tfidf_default.transform(df_val[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = df_train_augmented[\"label\"].values\n",
    "y_val = df_val[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]...*..*\n",
      "optimization finished, #iter = 5694\n",
      "obj = -413.500946, rho = -0.941106\n",
      "nSV = 4220, nBSV = 286\n",
      ".....*...*\n",
      "optimization finished, #iter = 8067\n",
      "obj = -872.366028, rho = -0.839806\n",
      "nSV = 5371, nBSV = 648\n",
      ".*\n",
      "optimization finished, #iter = 1396\n",
      "obj = -338.880396, rho = 0.751311\n",
      "nSV = 877, nBSV = 269\n",
      "Total nSV = 6461\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_default.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: SVC,\n",
    "    labels_train: np.ndarray,\n",
    "    labels_val: np.ndarray,\n",
    "    features_train: np.ndarray,\n",
    "    features_val: np.ndarray,\n",
    "    eval_object: TextClassificationEvaluator = evaluator,\n",
    "):\n",
    "    predictions_train = model.predict(features_train)\n",
    "    predictions_val = model.predict(features_val)\n",
    "    \n",
    "    train_metrics = evaluator.calculate_metrics(labels_train, predictions_train)\n",
    "    validation_metrics = evaluator.calculate_metrics(labels_val, predictions_val)\n",
    "    \n",
    "    return train_metrics, validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_train, metrics_val = evaluate_model(\n",
    "    model=svm_default,\n",
    "    labels_train=y_train,\n",
    "    labels_val=y_val,\n",
    "    features_train=X_train_tfidf,\n",
    "    features_val=X_val_tfidf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{'accuracy': 0.9624760689215061, 'f1_macro': 0.8420009498522497, 'f1_micro': 0.9624760689215061}\n",
      "\n",
      "--------------------\n",
      "\n",
      "Validation metrics:\n",
      "{'accuracy': 0.9215452011150936, 'f1_macro': 0.4065510938891632, 'f1_micro': 0.9215452011150936}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train metrics:\\n{metrics_train}\")\n",
    "print()\n",
    "print(\"-\"*20)\n",
    "print()\n",
    "print(f\"Validation metrics:\\n{metrics_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, the model run on default hyperparameters is strongly overfitted.\n",
    "\n",
    "In the hyperparameter tuning process, I will use f1_marco on validation dataset as the optimization target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_to_log = {}\n",
    "\n",
    "for metric, value in metrics_train.items():\n",
    "    new_key = metric + \"_train\"\n",
    "    metrics_to_log[new_key] = value\n",
    "    \n",
    "for metric, value in metrics_val.items():\n",
    "    new_key = metric + \"_val\"\n",
    "    metrics_to_log[new_key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/03 13:03:47 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "log_experiment(\n",
    "    experiment_name=\"Default_hyperparameters\",\n",
    "    metrics=metrics_to_log,\n",
    "    model=svm_default,\n",
    "    vectorizer=tfidf_default,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading hyperparameters ranges\n",
    "svm_hyperparams, tfidf_hyperparams = ConfigParser.load_hyperparams(\"hyperparameters.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial, \n",
    "    X_train, \n",
    "    X_test, \n",
    "    y_train, \n",
    "    y_test,\n",
    "    svm_hyperparams,\n",
    "    tfidf_hyperparams,\n",
    "    evaluator,\n",
    "):\n",
    "    svm_C = trial.suggest_loguniform('svm_c', *svm_hyperparams.C)\n",
    "    svm_kernel = trial.suggest_categorical('svm_kernel', svm_hyperparams.kernel)\n",
    "    svm_class_weight = svm_hyperparams.class_weight\n",
    "    tfidf_max_features = trial.suggest_int('tfidf_max_features', *tfidf_hyperparams.max_features)\n",
    "    tfidf_min_df = trial.suggest_int('tfidf_min_df', *tfidf_hyperparams.min_df)\n",
    "    tfidf_max_df = trial.suggest_uniform('tfidf_max_df', *tfidf_hyperparams.max_df)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=tfidf_max_df,\n",
    "        max_features=tfidf_max_features,\n",
    "        min_df=tfidf_min_df\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    svm = SVC(\n",
    "        C=svm_C,\n",
    "        kernel=svm_kernel,\n",
    "        class_weight=svm_class_weight\n",
    "    )\n",
    "    \n",
    "    svm.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    y_pred = svm.predict(X_test_tfidf)\n",
    "    \n",
    "    f1_macro = evaluator.calculate_f1_macro(y_test, y_pred)\n",
    "    \n",
    "    return f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = df_train_augmented[\"text\"].values\n",
    "X_val = df_val[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_callback(study, trial):\n",
    "    best_trial = study.best_trial\n",
    "    if trial.number % 10 == 0 or trial.number == 199:\n",
    "        print(f\"Trial {trial.number}: {trial.value} Best trial so far: {best_trial.number}: {best_trial.value}\")\n",
    "\n",
    "        log_experiment(\n",
    "            experiment_name=\"hyperparams_tuning\",\n",
    "            metrics={\"f1_macro\": best_trial.value},\n",
    "            hyperparams=best_trial.params,\n",
    "            run_name=f\"trial_{trial.number}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: 0.03757985719654265 Best trial so far: 0: 0.03757985719654265\n",
      "Trial 10: 0.4217346846256567 Best trial so far: 2: 0.4301355440304142\n",
      "Trial 20: 0.42054829993645715 Best trial so far: 17: 0.4589135397543051\n",
      "Trial 30: 0.40993178954134396 Best trial so far: 26: 0.46558393062556785\n",
      "Trial 40: 0.40802719172193536 Best trial so far: 31: 0.466316976213899\n",
      "Trial 50: 0.470975173187786 Best trial so far: 50: 0.470975173187786\n",
      "Trial 60: 0.47335940495360784 Best trial so far: 58: 0.481140752772768\n",
      "Trial 70: 0.4803065134099617 Best trial so far: 58: 0.481140752772768\n",
      "Trial 80: 0.47491313237317384 Best trial so far: 58: 0.481140752772768\n",
      "Trial 90: 0.40441337988636894 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 100: 0.481140752772768 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 110: 0.4009299026351485 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 120: 0.4447613946678073 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 130: 0.4800495375180718 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 140: 0.47990862298273745 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 150: 0.4776881875386929 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 160: 0.4767211103442988 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 170: 0.47393337566763644 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 180: 0.4557691300369702 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 190: 0.48049272382859587 Best trial so far: 88: 0.48179449757396525\n",
      "Trial 199: 0.47990862298273745 Best trial so far: 88: 0.48179449757396525\n"
     ]
    }
   ],
   "source": [
    "study.optimize(\n",
    "    lambda trial: objective(\n",
    "        trial,\n",
    "        X_train,\n",
    "        X_val,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        svm_hyperparams,\n",
    "        tfidf_hyperparams,\n",
    "        evaluator,\n",
    "    ),\n",
    "    n_trials=200,\n",
    "    callbacks=[log_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svm_c': 7.6495944342965165, 'svm_kernel': 'rbf', 'tfidf_max_features': 4692, 'tfidf_min_df': 1, 'tfidf_max_df': 0.518276935298537}\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "best_params = best_trial.params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_svm_hyperparams = {}\n",
    "best_tfidf_hyperparams = {}\n",
    "\n",
    "for key in best_params.keys():\n",
    "    if key.startswith(\"svm_\"):\n",
    "        best_svm_hyperparams[key.replace(\"svm_\", \"\")] = best_params[key]\n",
    "    elif key.startswith(\"tfidf_\"):\n",
    "        best_tfidf_hyperparams[key.replace(\"tfidf_\", \"\")] = best_params[key]\n",
    "\n",
    "best_svm_hyperparams[\"C\"] = best_svm_hyperparams[\"c\"]\n",
    "del best_svm_hyperparams[\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=7.6495944342965165, class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=7.6495944342965165, class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=7.6495944342965165, class_weight='balanced')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tfidf = TfidfVectorizer(**best_tfidf_hyperparams)\n",
    "best_svm = SVC(**best_svm_hyperparams, class_weight=\"balanced\")\n",
    "\n",
    "X_train_tfidf = best_tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = best_tfidf.transform(X_val)\n",
    "\n",
    "best_svm.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_metrics_train, best_metrics_val = evaluate_model(\n",
    "    model=best_svm,\n",
    "    labels_train=y_train,\n",
    "    labels_val=y_val,\n",
    "    features_train=X_train_tfidf,\n",
    "    features_val=X_val_tfidf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "{'accuracy': 0.9941289087428207, 'f1_macro': 0.9790723331833172, 'f1_micro': 0.9941289087428207}\n",
      "\n",
      "--------------------\n",
      "\n",
      "Validation metrics:\n",
      "{'accuracy': 0.9203504579848666, 'f1_macro': 0.48179449757396525, 'f1_micro': 0.9203504579848666}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train metrics:\\n{best_metrics_train}\")\n",
    "print()\n",
    "print(\"-\"*20)\n",
    "print()\n",
    "print(f\"Validation metrics:\\n{best_metrics_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning process improved results a little, but there is still strong overfitting. However, building the best possible solution is not the main purpose, so let's keep that results.\n",
    "\n",
    "By the way, the fact that accuracy is always equal to f1_micro is very interesting. The number of true positives, true negatives, false positives, and false negatives must be the same across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_metrics_to_log = {}\n",
    "\n",
    "for metric, value in best_metrics_train.items():\n",
    "    new_key = metric + \"_train\"\n",
    "    best_metrics_to_log[new_key] = value\n",
    "    \n",
    "for metric, value in best_metrics_val.items():\n",
    "    new_key = metric + \"_val\"\n",
    "    best_metrics_to_log[new_key] = value\n",
    "\n",
    "log_experiment(\n",
    "    experiment_name=\"Best_hyperparams\",\n",
    "    metrics=best_metrics_to_log,\n",
    "    model=best_svm,\n",
    "    vectorizer=best_tfidf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded best SVM and TFIDF objects and saved them in the /models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models/model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(\"models/vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "assert isinstance(model, SVC)\n",
    "assert isinstance(vectorizer, TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running best objects on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the evaluation process on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/test/test_set_only_text.txt\", \"r\") as f:\n",
    "    test_examples = f.readlines()\n",
    "\n",
    "with open(\"data/test/test_set_only_tags.txt\", \"r\") as f:\n",
    "    test_labels = f.readlines()\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    \"text\": test_examples,\n",
    "    \"label\": test_labels,\n",
    "})\n",
    "\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "df_test[\"label\"] = df_test[\"label\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@anonymized_account Spoko, jak im Duda z Morawieckim zamówią po pięć piw to wszystko będzie ok.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@anonymized_account @anonymized_account Ale on tu nie miał szans jej zagrania, a ta 'proba' to czysta prowizorka.\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anonymized_account No czy Prezes nie miał racji, mówiąc,ze to są zdradzieckie mordy? No czy nie miał racji?😁😁\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anonymized_account @anonymized_account Przecież to nawet nie jest przewrotka 😂\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@anonymized_account @anonymized_account Owszem podatki tak. Ale nie w takich okolicznościach. Czemu Małysza odpalili z teamu Orlen?\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    text  \\\n",
       "0                                      @anonymized_account Spoko, jak im Duda z Morawieckim zamówią po pięć piw to wszystko będzie ok.\\n   \n",
       "1                    @anonymized_account @anonymized_account Ale on tu nie miał szans jej zagrania, a ta 'proba' to czysta prowizorka.\\n   \n",
       "2                       @anonymized_account No czy Prezes nie miał racji, mówiąc,ze to są zdradzieckie mordy? No czy nie miał racji?😁😁\\n   \n",
       "3                                                      @anonymized_account @anonymized_account Przecież to nawet nie jest przewrotka 😂\\n   \n",
       "4  @anonymized_account @anonymized_account Owszem podatki tak. Ale nie w takich okolicznościach. Czemu Małysza odpalili z teamu Orlen?\\n   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: preprocessor.preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spoko duda morawieckim zamówią pięć piw ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miał szans zagrania proba czysta prowizorka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prezes miał racji mówiącze zdradzieckie mordy miał racji</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>przewrotka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>podatki tak takich okolicznościach małysza odpalili teamu orlen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              text  label\n",
       "0                       spoko duda morawieckim zamówią pięć piw ok      0\n",
       "1                      miał szans zagrania proba czysta prowizorka      0\n",
       "2         prezes miał racji mówiącze zdradzieckie mordy miał racji      0\n",
       "3                                                       przewrotka      0\n",
       "4  podatki tak takich okolicznościach małysza odpalili teamu orlen      0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(df_test[\"text\"].values)\n",
    "y_test = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_preds = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics values on test set:\n",
      "{'accuracy': 0.871, 'f1_macro': 0.4379092184593614, 'f1_micro': 0.871}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metrics values on test set:\\n{evaluator.calculate_metrics(y_test, y_test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance got even worse on previously unseen data. If we want to achieve better results, we would have to apply several overfitting prevention techniques. Getting more data would be a good idea, as we had only around 10 000 observations in the training dataset. Also it may be a good idea to do some research about different text augmentation techniques, and use an algorithm which is less overfitting-prone.\n",
    "\n",
    "However, let's consider the current model and vectorizer as prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
